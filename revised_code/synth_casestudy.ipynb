{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d50288-9d89-4e5e-97d5-247ba50e9688",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import essential libraries\n",
    "import numpy as np  # Numerical computations\n",
    "import pandas as pd  # Data manipulation and analysis\n",
    "import matplotlib.pyplot as plt  # Plotting and visualization\n",
    "import seaborn as sns  # Statistical data visualization\n",
    "\n",
    "# Import machine learning models from scikit-learn\n",
    "import sklearn.linear_model as lm  # Linear models (e.g., Logistic Regression, Ridge, Lasso)\n",
    "import sklearn.ensemble as en  # Ensemble models (e.g., Random Forest, Gradient Boosting)\n",
    "import sklearn.tree as tree  # Decision Tree models\n",
    "\n",
    "# Import utilities for data processing and model training\n",
    "from sklearn.model_selection import train_test_split  # Splitting data into training and test sets\n",
    "from sklearn.impute import KNNImputer  # Handling missing values using k-nearest neighbors imputation\n",
    "\n",
    "# Import additional utility packages\n",
    "import importlib  # Dynamic import and reloading of modules\n",
    "import pyreadr  # Reading R data files (e.g., .rds, .RData)\n",
    "\n",
    "import tqdm  # Progress bar for loops and iterations\n",
    "import black  # Code formatting for better readability\n",
    "\n",
    "#### Main package (custom module)\n",
    "import learn_w as learn  # Import a custom module named 'learn_w'\n",
    "\n",
    "# Reload the custom module to reflect any recent changes\n",
    "importlib.reload(learn)\n",
    "\n",
    "# Suppress warnings to keep the notebook output clean\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Load the Jupyter Black extension for auto-formatting Python code\n",
    "%load_ext jupyter_black\n",
    "\n",
    "# Set Seaborn visualization settings for better aesthetics\n",
    "sns.set(font_scale=1.25, style=\"whitegrid\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01192f15-d214-4325-84bb-5bf8344ed7b3",
   "metadata": {},
   "source": [
    "# Fetching Real MOUD Data & Set it up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5da584-3fb2-4538-b7fb-467916a5d364",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define relevant column names\n",
    "outcome_cols = [\"opioiduse12\", \"opioiduse24\"]  # Outcome variables at 12 and 24 months\n",
    "treatment_col = \"medicine_assigned\"  # Treatment assignment column\n",
    "discrete_cov = [\"xrace\", \"mar\", \"sex\"]  # Discrete covariates\n",
    "\n",
    "# Load harmonized baseline data\n",
    "baseline_harmonized = pd.read_csv(\n",
    "    \"/Users/harshparikh/Library/CloudStorage/OneDrive-JohnsHopkins/MOUD_data/updated_data/ctn0094/drv/clean_patients_with_relapse_wide.csv\",\n",
    "    index_col=0,\n",
    ")\n",
    "\n",
    "# Load multiple stacked datasets into a list\n",
    "stacked_list = []\n",
    "for i in range(1, 6):\n",
    "    stacked_list.append(\n",
    "        pd.read_csv(\n",
    "            \"/Users/harshparikh/Library/CloudStorage/OneDrive-JohnsHopkins/MOUD_data/stacked_list_%d.csv\"\n",
    "            % (i),\n",
    "            index_col=0,\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Select the first dataset from stacked list\n",
    "df = stacked_list[0]\n",
    "\n",
    "# Subset data from the TEDSA dataset (trialdata == 0)\n",
    "df_tedsa = df.loc[df[\"trialdata\"] == 0]\n",
    "\n",
    "# Extract patients from the CTN-0094 project\n",
    "ct94 = baseline_harmonized.loc[(baseline_harmonized[\"project\"] == 27)]\n",
    "outcome94 = ct94[outcome_cols]  # Extract outcome columns for this dataset\n",
    "\n",
    "# Identify common columns between the TEDSA dataset and CTN-0094 dataset\n",
    "common_cols = set.intersection(set(df_tedsa.columns), set(ct94.columns))\n",
    "\n",
    "# Subset the CTN-0094 dataset with common columns, dropping 'edu' and 'mar'\n",
    "ct94_cc = ct94[common_cols].drop(columns=[\"edu\", \"mar\"])\n",
    "\n",
    "# Convert 'sex' into a binary variable: male = 1, female = 0\n",
    "ct94_cc[\"sex\"] = (ct94[\"sex\"] == \"male\").astype(int)\n",
    "\n",
    "# Impute missing values using KNN imputer\n",
    "imputer = KNNImputer(n_neighbors=4, weights=\"distance\", add_indicator=False)\n",
    "ct94_cc_imputed = imputer.fit_transform(ct94_cc)\n",
    "\n",
    "# Convert imputed array back into DataFrame with original column names\n",
    "ct94_cc = pd.DataFrame(ct94_cc_imputed, index=ct94_cc.index, columns=ct94_cc.columns)\n",
    "\n",
    "# Convert treatment assignment to binary: methadone = 1, buprenorphine = 0\n",
    "ct94_cc[\"med_met\"] = (ct94[treatment_col] == \"met\").astype(int)\n",
    "\n",
    "# Drop any remaining NaN values\n",
    "ct94_cc = ct94_cc.dropna()\n",
    "\n",
    "# Assign sample indicator: CTN-0094 patients (S = 1)\n",
    "ct94_cc[\"S\"] = 1\n",
    "\n",
    "# Round numerical values and ensure they are stored as integers\n",
    "ct94_cc = ct94_cc.round(0).astype(int)\n",
    "\n",
    "# Merge outcome data with the CTN-0094 dataset\n",
    "ct94_cc = ct94_cc.join(outcome94, how=\"inner\")\n",
    "\n",
    "# Print the shape of the processed dataset\n",
    "print(ct94_cc.shape)\n",
    "\n",
    "# Compute and display mean outcome values by treatment group\n",
    "ct94_cc.groupby(by=\"med_met\").mean()[outcome_cols]\n",
    "\n",
    "# Prepare TEDSA dataset: Select common columns, drop 'edu' and 'mar'\n",
    "df_tedsa_cc = df_tedsa[common_cols].drop(columns=[\"edu\", \"mar\"])\n",
    "\n",
    "# Assign sample indicator: TEDSA patients (S = 0)\n",
    "df_tedsa_cc[\"S\"] = 0\n",
    "\n",
    "# Convert categorical age groups into approximate numeric age values\n",
    "df_tedsa_cc[\"age\"].replace(\n",
    "    {\n",
    "        1: 13,   # Youngest age category\n",
    "        2: 16,\n",
    "        3: 18,\n",
    "        4: 22,\n",
    "        5: 27,\n",
    "        6: 32,\n",
    "        7: 37,\n",
    "        8: 42,\n",
    "        9: 47,\n",
    "        10: 52,\n",
    "        11: 60,\n",
    "        12: 68,  # Oldest age category\n",
    "    },\n",
    "    inplace=True,\n",
    ")\n",
    "\n",
    "# Combine TEDSA and CTN-0094 datasets after shuffling TEDSA observations\n",
    "df_primary = pd.concat([df_tedsa_cc.sample(frac=1, replace=False), ct94_cc])\n",
    "\n",
    "# Drop the first outcome variable and replace NaNs with 0\n",
    "df_ = df_primary.drop(columns=[outcome_cols[0]]).fillna(0)\n",
    "\n",
    "# Define key analysis variables\n",
    "outcome = outcome_cols[1]  # Use the second outcome variable for analysis\n",
    "treatment = \"med_met\"  # Treatment indicator (methadone vs. buprenorphine)\n",
    "sample = \"S\"  # Sample indicator (TEDSA = 0, CTN-0094 = 1)\n",
    "data = df_\n",
    "\n",
    "# Extract specific columns for analysis\n",
    "S = df_[sample]  # Sample indicator\n",
    "Y = df_[outcome]  # Outcome variable\n",
    "T = df_[treatment]  # Treatment assignment\n",
    "\n",
    "# Convert categorical variables into dummy variables for regression analysis\n",
    "data_dummy = pd.get_dummies(data, columns=[\"xrace\"])\n",
    "\n",
    "# Rename dummy variables to more readable names\n",
    "data_dummy.rename(\n",
    "    columns={\n",
    "        \"sex\": \"Male\",\n",
    "        \"age\": \"Age\",\n",
    "        \"ivdrug\": \"IV Drug Use\",\n",
    "        \"bamphetamine30_base\": \"Hx Amphetamine\",\n",
    "        \"bbenzo30_base\": \"Hx Benzo\",\n",
    "        \"bcannabis30_base\": \"Hx Cannabis\",\n",
    "        \"xrace_1\": \"White\",\n",
    "        \"xrace_2\": \"Black\",\n",
    "        \"xrace_3\": \"Hispanic\",\n",
    "        \"xrace_4\": \"Other Race\",\n",
    "    },\n",
    "    inplace=True,\n",
    ")\n",
    "\n",
    "# Define predictor variables (excluding outcome, treatment, and sample indicators)\n",
    "X = data_dummy.drop(columns=[outcome, treatment, sample])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b316995-9b84-4639-853e-f972ac4ae9f8",
   "metadata": {},
   "source": [
    "# Generate Synthetic MOUD Data via Modeling\n",
    "Impute Y(t) \\\n",
    "Logistic regression to model P(S=1 | X), \\\n",
    "Logistic regression to model P(T=1 | X, S=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cd96f6-8283-447f-a76a-dcb8d12aa35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Fit Gradient Boosting classifiers to estimate P(Y | X, S=1, T=1) and P(Y | X, S=1, T=0)\n",
    "# - y1_m predicts the probability of Y=1 given X in the treated (T=1) group within S=1\n",
    "# - y0_m predicts the probability of Y=1 given X in the control (T=0) group within S=1\n",
    "\n",
    "y1_m = en.GradientBoostingClassifier(n_estimators=10000).fit(\n",
    "    X.loc[(S == 1) * (T == 1)], Y.loc[(S == 1) * (T == 1)]\n",
    ")\n",
    "y0_m = en.GradientBoostingClassifier(n_estimators=10000).fit(\n",
    "    X.loc[(S == 1) * (T == 0)], Y.loc[(S == 1) * (T == 0)]\n",
    ")\n",
    "\n",
    "# Estimate selection probability P(S=1 | X) using logistic regression with cross-validation\n",
    "pi_m = lm.LogisticRegressionCV().fit(X, S)\n",
    "\n",
    "# Estimate treatment probability P(T=1 | X, S=1) using logistic regression within S=1\n",
    "e_m = lm.LogisticRegressionCV().fit(X.loc[S == 1], T.loc[S == 1])\n",
    "\n",
    "# Create a copy of X for simulation\n",
    "X_sim = X.copy(deep=True)\n",
    "\n",
    "# Predict counterfactual outcomes using estimated probabilities from gradient boosting models\n",
    "joint_sim = X_sim.copy(deep=True)\n",
    "joint_sim[\"Y(1)\"] = np.random.binomial(1, y1_m.predict_proba(X_sim)[:, 1])  # Simulated outcome under treatment\n",
    "joint_sim[\"Y(0)\"] = np.random.binomial(1, y0_m.predict_proba(X_sim)[:, 1])  # Simulated outcome under control\n",
    "\n",
    "# Simulate sample selection using the estimated probability P(S=1 | X)\n",
    "S_sim = np.random.binomial(1, pi_m.predict_proba(X_sim)[:, 1])\n",
    "\n",
    "# Simulate treatment assignment using the estimated probability P(T=1 | X, S=1)\n",
    "T_sim = np.random.binomial(1, e_m.predict_proba(X_sim)[:, 1])\n",
    "\n",
    "# Simulate observed outcome based on treatment assignment\n",
    "Y_sim = T_sim * joint_sim[\"Y(1)\"] + (1 - T_sim) * joint_sim[\"Y(0)\"]\n",
    "\n",
    "# Construct the final simulated dataset\n",
    "df_sim = X_sim.copy(deep=True)\n",
    "df_sim[\"Y\"] = Y_sim  # Add simulated outcome\n",
    "df_sim[\"T\"] = T_sim  # Add simulated treatment assignment\n",
    "df_sim[\"S\"] = S_sim  # Add simulated sample membership"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e38fc55-dadd-4c37-bf4b-b3d940a06717",
   "metadata": {},
   "source": [
    "## Plot feature importance of variables for outcome and selection models from the synthetic DGP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7f551b-0b53-4fbf-8e21-14c865a10479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame to store feature importance values\n",
    "feature_imp = pd.DataFrame()\n",
    "\n",
    "# Compute the absolute feature importance scores for treatment effect estimation\n",
    "# - `y1_m.feature_importances_` corresponds to the treated group (T=1)\n",
    "# - `y0_m.feature_importances_` corresponds to the control group (T=0)\n",
    "# - Summing these gives an aggregate importance measure across both groups\n",
    "feature_imp[\"treatment effect\"] = pd.Series(\n",
    "    y1_m.feature_importances_ + y0_m.feature_importances_, index=X.columns\n",
    ").abs()\n",
    "\n",
    "# Compute feature importance for sample selection using logistic regression coefficients\n",
    "# - `pi_m.coef_[0]` contains the estimated logistic regression coefficients for selection (S=1)\n",
    "feature_imp[\"sample\"] = pd.Series(pi_m.coef_[0], index=X.columns).abs()\n",
    "\n",
    "# Normalize feature importance values using Min-Max Scaling (0 to 1 range)\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "feature_imp_scaled = pd.DataFrame(\n",
    "    scaler.fit_transform(feature_imp),\n",
    "    columns=feature_imp.columns,\n",
    "    index=feature_imp.index,\n",
    ")\n",
    "\n",
    "\n",
    "# Define function to label points on the scatter plot\n",
    "def label_point(data, x, y, val, ax):\n",
    "    for i in data.index:\n",
    "        # Adjust label positioning for 'Hispanic' to avoid overlap\n",
    "        if \"Hispanic\" in str(data.loc[i][val]):\n",
    "            ax.text(data.loc[i][x] + 0.01, data.loc[i][y], str(data.loc[i][val]))\n",
    "        else:\n",
    "            ax.text(data.loc[i][x] + 0.01, data.loc[i][y] - 0.05, str(data.loc[i][val]))\n",
    "\n",
    "\n",
    "# Compute treatment effect heterogeneity (squared deviation from mean TE)\n",
    "joint_sim[\"TE\"] = joint_sim[\"Y(1)\"] - joint_sim[\"Y(0)\"]  # Individual treatment effects\n",
    "joint_sim[\"h\"] = (joint_sim[\"TE\"] - joint_sim[\"TE\"].mean()) ** 2  # Squared deviation from mean TE\n",
    "\n",
    "# Fit Gradient Boosting Regressor to model treatment effect heterogeneity\n",
    "# This model estimates how much each feature contributes to variation in treatment effects\n",
    "te_exp = en.GradientBoostingRegressor().fit(X_sim, joint_sim[\"h\"])\n",
    "\n",
    "# Update treatment effect importance using the new heterogeneity model\n",
    "feature_imp[\"treatment effect\"] = pd.Series(\n",
    "    te_exp.feature_importances_, index=X_sim.columns\n",
    ").abs()\n",
    "\n",
    "# Normalize feature importance scores again after updating treatment effect importance\n",
    "scaler = MinMaxScaler()\n",
    "feature_imp_scaled = pd.DataFrame(\n",
    "    scaler.fit_transform(feature_imp),\n",
    "    columns=feature_imp.columns,\n",
    "    index=feature_imp.index,\n",
    ")\n",
    "\n",
    "\n",
    "# Define function to label points on the scatter plot\n",
    "def label_point(data, x, y, val, ax):\n",
    "    for i in data.index:\n",
    "        # Adjust label positioning for 'Hispanic' to avoid overlap\n",
    "        if \"Hispanic\" in str(data.loc[i][val]):\n",
    "            ax.text(data.loc[i][x] + 0.01, data.loc[i][y] - 0.05, str(data.loc[i][val]))\n",
    "        else:\n",
    "            ax.text(data.loc[i][x] + 0.01, data.loc[i][y], str(data.loc[i][val]))\n",
    "\n",
    "\n",
    "# Create scatter plot to compare feature importance for treatment effects and sample selection\n",
    "fig, ax = plt.subplots(figsize=(10, 7), dpi=600)\n",
    "\n",
    "sns.scatterplot(\n",
    "    data=np.log2(feature_imp_scaled + 1).reset_index(),  # Log transformation to improve visualization\n",
    "    x=\"treatment effect\",\n",
    "    y=\"sample\",\n",
    "    hue=\"index\",  # Color by feature name\n",
    "    ax=ax,\n",
    "    s=100,  # Size of points\n",
    "    legend=False,  # Hide legend for clarity\n",
    ")\n",
    "\n",
    "# Add text labels to scatter points\n",
    "label_point(\n",
    "    data=np.log2(feature_imp_scaled + 1).reset_index(),\n",
    "    x=\"treatment effect\",\n",
    "    y=\"sample\",\n",
    "    val=\"index\",\n",
    "    ax=ax,\n",
    ")\n",
    "\n",
    "# Set axis labels\n",
    "plt.xlabel(\"Relative Feature Importance\\n (Treatment Effect)\")\n",
    "plt.ylabel(\"Relative Feature Importance\\n (Sample Selection Function)\")\n",
    "\n",
    "# plt.legend(ncols=3, loc=(0, -0.35))  # Commented out to reduce clutter\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save figure as PDF\n",
    "plt.savefig(\"feature_importance_synth_case.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93b444a-c63a-4708-a6e2-91495671d2f0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef099a34-9111-49aa-9773-5aeee3a4165a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = df_sim  # Assign the simulated dataset for further analysis\n",
    "treatment = \"T\"  # Treatment assignment column\n",
    "outcome = \"Y\"  # Outcome variable column\n",
    "sample = \"S\"  # Sample indicator column (S=1 for one dataset, S=0 for another)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3e3e99-d5ed-414d-ba91-549e7ae997e0",
   "metadata": {},
   "source": [
    "## Estimate Treatment Effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757b7428-3c9e-47b7-87f1-c2b3badcee6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Reload the custom module `learn` (useful if making modifications)\n",
    "importlib.reload(learn)\n",
    "\n",
    "# Estimate inverse probability weighting (IPW) treatment effects using the custom function\n",
    "df_v_est, pi_est, pi_m_est, e_m_est, data2_est = learn.estimate_ipw(\n",
    "    data, outcome, treatment, sample\n",
    ")\n",
    "\n",
    "# Print ATE (Average Treatment Effect) estimated from the randomized controlled trial (RCT)\n",
    "# This is the difference in mean outcomes between treated and control groups in S=1\n",
    "print(\n",
    "    \"RCT-ATE: %.2f ± %.2f\"\n",
    "    % (\n",
    "        100\n",
    "        * (\n",
    "            data.loc[(data[sample] == 1) * (data[treatment] == 1), outcome].mean()\n",
    "            - data.loc[(data[sample] == 1) * (data[treatment] == 0), outcome].mean()\n",
    "        ),\n",
    "        100\n",
    "        * (\n",
    "            data.loc[(data[sample] == 1) * (data[treatment] == 1), outcome].sem()\n",
    "            + data.loc[(data[sample] == 1) * (data[treatment] == 0), outcome].sem()\n",
    "        ),\n",
    "    )\n",
    ")\n",
    "\n",
    "# Print IPW-corrected ATE from the randomized controlled trial (RCT)\n",
    "# This accounts for covariate imbalance using inverse probability weighting (IPW)\n",
    "print(\n",
    "    \"RCT-IPW ATE: %.2f ± %.2f\" % (100 * df_v_est[\"a\"].mean(), 100 * df_v_est[\"a\"].sem())\n",
    ")\n",
    "\n",
    "# Print the transported ATE (generalized to S=0 population)\n",
    "# This applies IPW-based transportability methods to adjust for sample selection bias\n",
    "print(\n",
    "    \"Transported ATE: %.2f ± %.2f\"\n",
    "    % (100 * df_v_est[\"te\"].mean(), 100 * df_v_est[\"te\"].sem())\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03dace9d-4dc0-4c5a-9793-f0c6f3f6d61f",
   "metadata": {},
   "source": [
    "## Plot Selection Score per Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a78b56-8a56-4d15-8a45-183d36bdf51e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create a deep copy of the data for logistic regression-based propensity score adjustments\n",
    "data_dummy_logit = data.copy(deep=True)\n",
    "\n",
    "# Compute estimated selection score P(S=1 | X) using the fitted logistic regression model\n",
    "data_dummy_logit[\"pi(x)\"] = pi_m_est.predict_proba(X_sim)[:, 1]\n",
    "\n",
    "# Compute the stabilized inverse probability weight: pi(x) / pi\n",
    "# - This adjustment ensures weights are not too extreme and reduces variance\n",
    "data_dummy_logit[\"pi(x)/pi\"] = data_dummy_logit[\"pi(x)\"] / data_dummy_logit[\"S\"].mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bdd35d4-33b0-49be-a605-917e61f1da93",
   "metadata": {},
   "source": [
    "## Learn Underrepresented Groups via 3 different proposed methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02cbe2d-6ade-425e-abe6-6fb5808f62a2",
   "metadata": {},
   "source": [
    "### Indicator Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641e4125-b653-485a-8bf2-9b005f15a98d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "importlib.reload(learn)\n",
    "np.random.seed(42)\n",
    "D_brute, f_brute, _ = learn.kmeans_opt(\n",
    "    data=data,\n",
    "    outcome=outcome,\n",
    "    treatment=treatment,\n",
    "    sample=sample,\n",
    ")\n",
    "print(\n",
    "    (\n",
    "        100 * D_brute.loc[D_brute[\"w\"].astype(int) == 1][\"v\"].mean(),\n",
    "        100 * D_brute.loc[D_brute[\"w\"].astype(int) == 1][\"v\"].sem(),\n",
    "    )\n",
    ")\n",
    "\n",
    "tree.plot_tree(f_brute, feature_names=X_sim.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c01a7ae-cd34-4288-9a75-bfb82c91ebf5",
   "metadata": {},
   "source": [
    "### Linear Approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f124a22-af7e-4252-a63e-5e2365a1fdd6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "importlib.reload(learn)\n",
    "np.random.seed(42)\n",
    "D_linear, f_linear, _ = learn.linear_opt(\n",
    "    data=data_dummy_logit,\n",
    "    outcome=outcome,\n",
    "    treatment=treatment,\n",
    "    sample=sample,\n",
    ")\n",
    "print(\n",
    "    (\n",
    "        100 * D_linear.loc[D_linear[\"w\"].astype(int) == 1][\"v\"].mean(),\n",
    "        100 * D_linear.loc[D_linear[\"w\"].astype(int) == 1][\"v\"].sem(),\n",
    "    )\n",
    ")\n",
    "\n",
    "tree.plot_tree(f_linear, feature_names=X_sim.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b20197f-d96a-40bf-8f03-ae90a7ab3315",
   "metadata": {},
   "source": [
    "### Using a Single Tree Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c790fe6-d3ef-4d3b-8960-355d74bc3c6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "importlib.reload(learn)\n",
    "np.random.seed(366)\n",
    "D_tree, f_tree, _ = learn.tree_opt(\n",
    "    data=data,\n",
    "    outcome=outcome,\n",
    "    treatment=treatment,\n",
    "    sample=sample,\n",
    "    leaf_proba=0.1,\n",
    ")\n",
    "print(\n",
    "    (\n",
    "        100 * D_tree.loc[D_tree[\"w\"].astype(int) == 1][\"v\"].mean(),\n",
    "        100 * D_tree.loc[D_tree[\"w\"].astype(int) == 1][\"v\"].sem(),\n",
    "    )\n",
    ")\n",
    "\n",
    "tree.plot_tree(f_tree, feature_names=X_sim.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d428aae2-045d-4447-b5cd-6dcb85e8c73f",
   "metadata": {},
   "source": [
    "### Using ROOT based Forest Optimizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339a66be-4c5c-4d07-bedb-a799172360bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "importlib.reload(learn)\n",
    "np.random.seed(0)\n",
    "D_rash, D_forest, w_forest, rashomon_set, f_forest, _ = learn.forest_opt(\n",
    "    data=data,\n",
    "    outcome=outcome,\n",
    "    treatment=treatment,\n",
    "    sample=sample,\n",
    "    num_trees=2000,\n",
    "    vote_threshold=99 / 100,\n",
    "    explore_proba=0.1,\n",
    "    feature_est=\"gbt\",\n",
    "    top_k_trees=1,\n",
    ")\n",
    "print(\n",
    "    (\n",
    "        100 * D_rash.loc[D_rash[\"w_opt\"].astype(int) == 1][\"v\"].mean(),\n",
    "        100 * D_rash.loc[D_rash[\"w_opt\"].astype(int) == 1][\"v\"].sem(),\n",
    "    )\n",
    ")\n",
    "\n",
    "# tree.plot_tree(f_forest, feature_names=X_sim.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba911c7-9c34-4098-bdda-9cdc52e6ccc4",
   "metadata": {},
   "source": [
    "### Plotting ROOT Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec92d185-2b21-4fda-85b3-5e28bb02ba64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "baseline_loss = np.sqrt(np.sum(D_forest[\"vsq\"]) / ((D_forest.shape[0] ** 2)))\n",
    "local_obj = pd.DataFrame(\n",
    "    np.array([w_forest[i][\"local objective\"] for i in range(len(w_forest))]),\n",
    "    columns=[\"Objective\"],\n",
    ").sort_values(by=\"Objective\")\n",
    "\n",
    "# top_k = 1\n",
    "# # sns.pointplot((local_obj.iloc[:top_k])[\"Objective\"].values)\n",
    "\n",
    "\n",
    "w_rash = [\n",
    "    \"w_tree_%d\" % (i)\n",
    "    for i in range(len(w_forest))\n",
    "    if i in list(local_obj.iloc[:top_k].index)\n",
    "]\n",
    "avg_votes = (D_forest[w_rash].mean(axis=1) >= 0.99).astype(int)\n",
    "D_rash[\"w_opt\"] = avg_votes\n",
    "\n",
    "np.random.seed(42)\n",
    "num_trees = 1\n",
    "explainer = tree.DecisionTreeClassifier(max_depth=3).fit(\n",
    "    X.loc[avg_votes.index], avg_votes\n",
    ")\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(nrows=num_trees, figsize=(20, 8), dpi=600)\n",
    "for i in range(num_trees):\n",
    "    if num_trees == 1:\n",
    "        tree.plot_tree(\n",
    "            explainer,  # .estimators_[i, 0],\n",
    "            feature_names=X.columns,\n",
    "            ax=ax,\n",
    "            filled=True,\n",
    "            fontsize=10,\n",
    "            # proportion=True,\n",
    "            impurity=False,\n",
    "        )\n",
    "    else:\n",
    "        tree.plot_tree(\n",
    "            explainer.estimators_[i, 0],\n",
    "            feature_names=X.columns,\n",
    "            ax=ax[i],\n",
    "            filled=True,\n",
    "            fontsize=10,\n",
    "            # proportion=True,\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
